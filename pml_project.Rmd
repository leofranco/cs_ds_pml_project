---
title: "pml_project"
author: "Leo Franco"
date: "5 Mar 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
library(readr)
library(caret)
```

## Peer-graded Assignment: Prediction Assignment Writeup

### Objective

"The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases."

Remmeber these are the points:

* Create a report describing how you built your model 
* How you used cross validation
* What you think the expected out of sample error is
* Why you made the choices you did. 
* You will also use your prediction model to predict 20 different test cases

### Read the data

```{r, warning=FALSE, message=FALSE}
data_training <- read_csv("pml-training.csv")
data_testing <- read_csv("pml-testing.csv")
#names(data_training)
```

```{r}
dim(data_training)
dim(data_testing)
```

We see there are 19622 rows with 160 columns. One of the columns is the
variable *classe*, which is what we will try to predict from the others.

We can see what is variable about:
```{r}
table(data_training$classe)
```

### Clean the data

We can see there are a lot of coumns that wouldn't be useful in a prediction,
for instance a timestamp or the user name. Let's try to find the columns
we need.

There is a useful function in caret. It will find the columns with no variance.
That means the columns that do not change even if otehr variable do change. 
We will remove those columns

```{r}
data_training2 <- data_training[,-nearZeroVar(data_training)]
data_testing2 <- data_testing[,-nearZeroVar(data_training)]
```

We can remove the first 6 columns that don't seem to have anything useful.

```{r}
data_training3 <- data_training2[,-c(1:6)]
data_testing3 <- data_testing2[,-c(1:6)]
```

There are a lot of columns with a lot of NAs. I don't know the data
well enough to know what is going on in detail so I am going to
remove all columns that have NAs.

```{r}
data_training4 <- data_training3[colSums(is.na(data_training3))/dim(data_training3)[1]<0.75]
data_testing4 <- data_testing3[colSums(is.na(data_training3))/dim(data_training3)[1]<0.75]

data_training4 <- data_training3[ colSums(is.na(data_training3))==0 ]
data_testing4 <- data_testing3[ colSums(is.na(data_training3))==0 ]

dim(data_training4)
dim(data_testing4)
```

### Create a model (random forest with a k=10 cross validation)

We can now create our model based on the clean dataset. Note that we also
exclude *na's* from the set.

Since we are using the *train* function in the *caret* package, out 
crossvalidation can be done automatically. We just have to specify it.

We are choosing a random forest model model since the output data is a
factor with 5 categories (some of the input variables might be too). The
data is pretty big so I am choosing a large number of cross validations.

```{r, cache=TRUE}
model_rf <- train(classe~.,data=data_training4,method="rf",trControl=trainControl(method="cv",number=10))
model_rf
model_rf$finalModel
```

From looking into the model we see the model with accuracy 0.99
with mtry=2 was selected. With an accuracy that high we expect the
error rate out of sample to be very low.

So far we can say we selected a model with a random forest and we used cross validation with k=10.

Once we have a model, we can use it to predict the outcomes in 
our testing data (which is out of sample)

### Predict the output for the testing dataset using the model.

```{r}
predicted <- predict(model_rf,data_testing4)
predicted
```

The output from the prediction is what we will have to use
for the last quiz of the assignment.

### Important variables

It looks like we ended up with a lot of variables.

There is a handy function that allows us to see which ones
are the most important.

```{r}
variableImportance <- varImp(model_rf)
plot(variableImportance)
```